# How to Use PrERT-CNM

This document provides a comprehensive guide on utilizing the deliverables produced over the four-month lifecycle of the PrERT-CNM project. It outlines expected results, execution commands, and local deployment strategies.

## Deployment Strategy and Environment Setup

The PrERT-CNM engine is designed as a modular Python package. To deploy the system locally or on a continuous integration server, follow these steps to bootstrap the environment.

### 1. Initialize Virtual Environment
It is highly recommended to isolate dependencies.
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install Dependencies
Install the required machine learning, probabilistic, and data processing libraries.
```bash
pip install -r requirements.txt
```

### 3. Handle HuggingFace Caching
By default, HuggingFace Hub attempts to cache heavy datasets and models in your global user directory. We recommend scoping this to the project directory to prevent system-wide permission errors.
```bash
export HF_HOME="$(pwd)/.hf_cache"
```

---

## Month-by-Month Usage Guide

### Month 1: Standards Mapping
**Component:** `config/` layer.

**How to Use:**
The primary deliverable is the deterministic mapping of privacy regulations to software configurations. You can interact with the current state of mappings by editing `config/privacy_indicators.json`.

**Expected Result:**
When executed through the loader, the system rigidly enforces GDPR/NIST structural integrity using Pydantic models. Any malformed mappings will crash the pipeline immediately, protecting the Bayesian Engine from processing invalid data.

**Commands:**
To verify the configuration logic holds true against the mappings:
```bash
pytest tests/test_pipeline.py -k "test_config_validation"
```

### Month 2: Metrics Definition & Synthetic Data
**Component:** `data/` layer.

**How to Use:**
The goal is to provide the models with data to learn from. The downloaded OPP-115 alternative public mirror is cached as the definitive baseline truth.

**Expected Result:**
Running the dataset download script caches the Parquet binary files into `data/raw/`. Data loading scripts will interface strictly with this local cache to avoid external HTTP dependencies during offline training.

**Commands:**
To pull and persist the data locally:
```bash
python data/download.py
```

### Month 3: AI Prototype Development
**Components:** `models/` and `engine/` layers.

**How to Use:**
This phase activates the core AI capabilities. You provide raw, unstructured privacy text to the `PrivacyFeatureExtractor` (transformer model). The logits generated by the model are passed to the `BayesianRiskEngine` as evidence nodes.

**Expected Result:**
The transformer layer extracts latent features representing clauses (e.g., data minimization principles). The Bayesian Network takes those features, updates its Variable Elimination distributions, and returns a concrete, auditable probability scale representing overall standard non-compliance risk.

**Commands:**
To run logic checks proving the topological mapping dynamically aligns the JSON config points to mathematically queryable DAG structures:
```bash
pytest tests/test_pipeline.py -k "test_dynamic_topology_generation"
```

### Month 4: Testing & Final Validation
**Component:** `tests/` and Benchmarking.

**How to Use:**
Execution of the complete CI/CD test suite simulating adversarial compliance anomalies and ensuring probabilistic limits.

**Expected Result:**
All unit tests should pass indicating that the risk indicators are mapped perfectly, uncertainty bounds are correctly mathematically restricted, and the HuggingFace trainers correctly compute tokenized arrays.

**Commands:**
Run the complete testing suite ensuring no module caching conflicts:
```bash
pytest tests/test_pipeline.py -v -p no:cacheprovider
```

<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>how_to_use - Masters</title>
        <link rel="stylesheet" href="..\..\css\styles.css" />
        <style>
            :root {
                --accent-color: #b366ff;
                --accent-text-color: #000000;
            }
        </style>
    </head>
    <body>
        <header>
            <button id="sidebar-toggle" aria-label="Toggle Navigation">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </button>
            <h1>Masters</h1>
        </header>
        <div class="container">
            <nav id="sidebar">
                <ul>
                    <li>
                        <a href="../../index.html">Home</a>
                    </li>
                    <li>
                        <a href="Sprint_1_Report.html">Sprint 1 Report</a>
                    </li>
                    <li>
                        <a href="Interactive_Showcase.html"
                            >Interactive Showcase</a
                        >
                    </li>
                    <li>
                        <a href="Architecture-Stack.html">Architecture Stack</a>
                    </li>
                    <li>
                        <a href="Funding_Proposal.html">Funding Proposal</a>
                    </li>
                    <li>
                        <a href="how_to_use.html"
                            ><strong>How To Use</strong></a
                        >
                    </li>
                    <li>
                        <a href="Model.html">Model</a>
                    </li>
                    <li><a href="README.html">README (Docs)</a></li>
                    <li><a href="../CHANGELOG.html">CHANGELOG</a></li>
                    <li><a href="../MEMORY.html">MEMORY</a></li>
                    <li><a href="../README.html">README (Root)</a></li>
                </ul>
            </nav>
            <main>
                <h1>How to Use PrERT-CNM</h1>
                <p>
                    This document provides a comprehensive guide on utilizing the deliverables produced over the four-month lifecycle of the PrERT-CNM project. It outlines expected results, execution commands, and local deployment strategies.
                </p>

                <h2>Deployment Strategy and Environment Setup</h2>
                <p>
                    The PrERT-CNM engine is designed as a modular Python package and includes a FastAPI backend for the showcase application. Below are the steps to deploy the system both locally for development and on a production server.
                </p>

                <h3>1. Initialize Environment</h3>
                <p>It is highly recommended to isolate dependencies using a virtual environment.</p>
                <pre><code class="language-bash">python3 -m venv .venv
# On macOS/Linux:
source .venv/bin/activate
# On Windows:
.venv\Scripts\activate
</code></pre>

                <h3>2. Install Dependencies</h3>
                <p>
                    Install the required machine learning, probabilistic, data processing, and web server libraries.
                </p>
                <pre><code class="language-bash">pip install -r requirements.txt
</code></pre>

                <h3>3. Configuration (.env)</h3>
                <p>
                    The ingestion layer relies on a Vector Database (ChromaDB) for Context Memory. Create a <code>.env</code> file in the root directory to configure the environment variables:
                </p>
                <pre><code class="language-env">CHROMA_COLLECTION_NAME=prert_memory
CHROMA_API_KEY=your_api_key_here
CHROMA_TENANT=default_tenant
CHROMA_DATABASE=default_database
</code></pre>

                <h3>4. Running Locally (Development)</h3>
                <p>
                    To run the evaluation engine and showcase API locally with hot-reloading:
                </p>
                <pre><code class="language-bash">uvicorn api.showcase_server:app --reload --host localhost --port 8000
</code></pre>
                <p>
                    The API will be available at <code>http://127.0.0.1:8000</code>. You can verify the server is running by visiting the health check endpoint:
                </p>
                <pre><code class="language-bash">curl http://127.0.0.1:8000/health
</code></pre>
                <p>
                    When using the Interactive Showcase HTML frontend, set the <strong>Backend API Server</strong> field to <code>http://127.0.0.1:8000</code>.
                </p>

                <h3>5. Server Deployment (Production)</h3>
                <p>
                    For production environments, it is recommended to run the server using <code>gunicorn</code> with <code>uvicorn</code> workers to handle concurrent requests robustly.
                </p>
                <p>
                    Ensure HuggingFace caching is properly scoped to avoid system-wide permission errors:
                </p>
                <pre><code class="language-bash">export HF_HOME=&quot;$(pwd)/.hf_cache&quot;
</code></pre>
                <p>
                    Start the production server:
                </p>
                <pre><code class="language-bash">gunicorn api.showcase_server:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
</code></pre>

                <h3>6. Exposing the Local Server with Cloudflare Tunnel</h3>
                <p>
                    When running the PrERT-CNM server on your local PC and needing to expose it to the internet (e.g. for the hosted GitHub Pages frontend to reach the backend), you can use <strong>Cloudflare Tunnel</strong> (<code>cloudflared</code>). This avoids the <strong>524 timeout error</strong> that occurs when intermediate proxies or tunnelling protocols are not configured to allow long-running AI inference requests.
                </p>

                <h4>Why This Is Needed</h4>
                <p>
                    The PrERT-CNM pipeline runs DeBERTa encoding, attention rollout, and CNM agent reasoning sequentially. For larger documents this process can exceed the default timeout thresholds of many reverse proxies and hosting platforms, resulting in a <strong>HTTP 524 (A Timeout Occurred)</strong> error. Running the server locally and tunnelling it via <code>cloudflared</code> bypasses these third-party timeout limits.
                </p>

                <h4>Install Cloudflared</h4>
                <p>
                    Download and install <code>cloudflared</code> for your platform:
                </p>
                <ul>
                    <li>
                        <p><strong>macOS (Homebrew):</strong></p>
                        <pre><code class="language-bash">brew install cloudflared</code></pre>
                    </li>
                    <li>
                        <p><strong>Windows (winget):</strong></p>
                        <pre><code class="language-bash">winget install --id Cloudflare.cloudflared</code></pre>
                    </li>
                    <li>
                        <p><strong>Linux (Debian/Ubuntu):</strong></p>
                        <pre><code class="language-bash">curl -L https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb -o cloudflared.deb
sudo dpkg -i cloudflared.deb</code></pre>
                    </li>
                </ul>

                <h4>Start the Tunnel</h4>
                <p>
                    First, make sure the uvicorn server is running locally:
                </p>
                <pre><code class="language-bash">uvicorn api.showcase_server:app --reload --host localhost --port 8000
</code></pre>
                <p>
                    Then, in a <strong>separate terminal</strong>, start the Cloudflare quick tunnel:
                </p>
                <pre><code class="language-bash">cloudflared tunnel --url http://localhost:8000
</code></pre>
                <p>
                    <code>cloudflared</code> will output a public URL (e.g. <code>https://&lt;random-subdomain&gt;.trycloudflare.com</code>). Copy this URL and paste it into the <strong>Backend API Server</strong> field in the Interactive Showcase frontend, or use it as the base URL for API requests.
                </p>

                <h4>Verifying the Tunnel</h4>
                <p>
                    Confirm the tunnel is working by calling the health endpoint through the public URL:
                </p>
                <pre><code class="language-bash">curl https://&lt;random-subdomain&gt;.trycloudflare.com/health
</code></pre>
                <p>
                    You should receive <code>{&quot;status&quot;:&quot;ok&quot;,&quot;pipeline&quot;:&quot;initialized&quot;}</code>.
                </p>

                <h4>Troubleshooting the 524 Timeout</h4>
                <p>
                    If you still encounter a 524 error:
                </p>
                <ol>
                    <li><strong>Ensure the local server is running</strong> — confirm <code>uvicorn</code> is active and responding on <code>http://localhost:8000/health</code>.</li>
                    <li><strong>Check cloudflared is connected</strong> — the terminal running <code>cloudflared</code> should show an active connection with no errors.</li>
                    <li><strong>Reduce document size</strong> — very large documents increase inference time. Try a shorter policy text first to confirm the pipeline works end-to-end.</li>
                    <li><strong>Check firewall rules</strong> — ensure your local firewall is not blocking connections on port 8000.</li>
                </ol>

                <hr />

                <h2>Month-by-Month Usage Guide</h2>

                <h3>Month 1: Standards Mapping</h3>
                <p><strong>Component:</strong> <code>config/</code> layer.</p>
                <p>
                    <strong>How to Use:</strong> The primary deliverable is the deterministic mapping of privacy regulations to software configurations. You can interact with the current state of mappings by editing <code>config/privacy_indicators.json</code>.
                </p>
                <p>
                    <strong>Expected Result:</strong> When executed through the loader, the system rigidly enforces GDPR/NIST structural integrity using Pydantic models. Any malformed mappings will crash the pipeline immediately, protecting the Bayesian Engine from processing invalid data.
                </p>
                <p>
                    <strong>Commands:</strong> To verify the configuration logic holds true against the mappings:
                </p>
                <pre><code class="language-bash">pytest tests/test_pipeline.py -k &quot;test_config_validation&quot;
</code></pre>

                <h3>Month 2: Metrics Definition &amp; Synthetic Data</h3>
                <p><strong>Component:</strong> <code>data/</code> layer.</p>
                <p>
                    <strong>How to Use:</strong> The goal is to provide the models with data to learn from. The downloaded OPP-115 alternative public mirror is cached as the definitive baseline truth.
                </p>
                <p>
                    <strong>Expected Result:</strong> Running the dataset download script caches the Parquet binary files into <code>data/raw/</code>. Data loading scripts will interface strictly with this local cache to avoid external HTTP dependencies during offline training.
                </p>
                <p>
                    <strong>Commands:</strong> To pull and persist the data locally:
                </p>
                <pre><code class="language-bash">python data/download.py
</code></pre>

                <h3>Month 3: AI Prototype Development</h3>
                <p>
                    <strong>Components:</strong> <code>models/</code> and <code>engine/</code> layers.
                </p>
                <p>
                    <strong>How to Use:</strong> This phase activates the core AI capabilities. You provide raw, unstructured privacy text to the <code>PrivacyFeatureExtractor</code> (transformer model). The logits generated by the model are passed to the <code>BayesianRiskEngine</code> as evidence nodes.
                </p>
                <p>
                    <strong>Expected Result:</strong> The transformer layer extracts latent features representing clauses (e.g., data minimization principles). The Bayesian Network takes those features, updates its Variable Elimination distributions, and returns a concrete, auditable probability scale representing overall standard non-compliance risk.
                </p>
                <p>
                    <strong>Commands:</strong> To run logic checks proving the topological mapping dynamically aligns the JSON config points to mathematically queryable DAG structures:
                </p>
                <pre><code class="language-bash">pytest tests/test_pipeline.py -k &quot;test_dynamic_topology_generation&quot;
</code></pre>

                <h3>Month 4: Testing &amp; Final Validation</h3>
                <p>
                    <strong>Component:</strong> <code>tests/</code> and Benchmarking.
                </p>
                <p>
                    <strong>How to Use:</strong> Execution of the complete CI/CD test suite simulating adversarial compliance anomalies and ensuring probabilistic limits.
                </p>
                <p>
                    <strong>Expected Result:</strong> All unit tests should pass indicating that the risk indicators are mapped perfectly, uncertainty bounds are correctly mathematically restricted, and the HuggingFace trainers correctly compute tokenized arrays.
                </p>
                <p>
                    <strong>Commands:</strong> Run the complete testing suite ensuring no module caching conflicts:
                </p>
                <pre><code class="language-bash">pytest tests/test_pipeline.py -v -p no:cacheprovider
</code></pre>
            </main>
        </div>
        <footer>
            <p>Generated by MD to Deploy</p>
        </footer>
        <script>
            document.addEventListener("DOMContentLoaded", () => {
                const toggle = document.getElementById("sidebar-toggle");
                const sidebar = document.getElementById("sidebar");

                if (toggle && sidebar) {
                    toggle.addEventListener("click", () => {
                        sidebar.classList.toggle("active");
                    });

                    // Optional: Close sidebar when clicking outside on mobile
                    document.addEventListener("click", (e) => {
                        if (
                            window.innerWidth <= 768 &&
                            sidebar.classList.contains("active") &&
                            !sidebar.contains(e.target) &&
                            !toggle.contains(e.target)
                        ) {
                            sidebar.classList.remove("active");
                        }
                    });
                }
            });
        </script>
    </body>
</html>
